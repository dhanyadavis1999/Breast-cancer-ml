# -*- coding: utf-8 -*-
"""Breast cancer -22014216.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1KJ4MToFle_eGQMQ-mjaQFXeZVfoxlSix

Import necessary libraries
"""

!pip install ucimlrepo
import pandas as pd
import numpy as np
from ucimlrepo import fetch_ucirepo
from sklearn.decomposition import PCA
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier
from sklearn.metrics import classification_report, accuracy_score, confusion_matrix, ConfusionMatrixDisplay, roc_auc_score, roc_curve
from sklearn.ensemble import IsolationForest
import matplotlib.pyplot as plt
import seaborn as sns
from xgboost import XGBClassifier
from scipy.stats import zscore

"""Fetch and load dataset"""

# Load the UCI breast cancer dataset
breast_cancer_data = fetch_ucirepo(id=17)

# Extract features and target
X = breast_cancer_data.data.features
y = breast_cancer_data.data.targets

# Display dataset metadata
print("Dataset Metadata:")
print(breast_cancer_data.metadata)
print(X)
print(y)

# Check for missing values,It returns a Data as X
print("Missing values in each column:\n", X.isnull().sum())
print("Missing values in target class:\n", y.isnull().sum())

"""no missing values in both data and target class

Exploratory Data Analysis

Print Target class distribution
"""

# Check the target class distribution,This function is applied to the target variable y, which contains the class labels (e.g., malignant or benign for the breast cancer dataset).
print("\nTarget class distribution:\n", y.value_counts())

"""Printing the datatypes of feature values"""

X.dtypes

X.describe

# Visualize the distribution of target classes
plt.figure(figsize=(6, 4))
sns.countplot(x=y['Diagnosis'])
plt.title("Distribution of Target Classes (Benign vs Malignant)")
plt.xlabel("Diagnosis (B = Benign, M = Malignant)")
plt.ylabel("Count")
plt.show()

"""So it is evident that it has a class imbalance."""

# Step 1: Visualize the distribution of a key feature (e.g., 'radius1')
plt.figure(figsize=(8, 6))
sns.histplot(X['radius1'], bins=30, kde=True, color='blue')
plt.xlabel("Radius1")
plt.ylabel("Frequency")
plt.show()

#Example of features to plot for the pairplot
features_to_plot = ['radius1', 'texture1', 'perimeter1', 'area1']
print(X.columns)
df_pairplot = X[features_to_plot].join(y["Diagnosis"])
sns.pairplot(df_pairplot, hue='Diagnosis', palette="Set1", diag_kind="kde")
plt.suptitle("Pairplot of Selected Features", y=1.02)
plt.show()

# Correlation heatmap to check relationships between variables
plt.figure(figsize=(12, 10))
sns.heatmap(X.corr(), cmap="coolwarm", annot=False, linewidths=0.5)
plt.title("Correlation Heatmap of Features")
plt.show()

# Calculate the correlation matrix
corr_matrix = X.corr()

# Find pairs of highly correlated features
high_corr = corr_matrix.unstack().sort_values(ascending=False)
high_corr_pairs = high_corr[(high_corr.abs() > 0.8) & (high_corr < 1.0)]

print(high_corr_pairs)

"""Preprocessing dataset

Drop highly correlated features
"""

# Drop the columns
X.drop(["radius1", "radius3", "compactness2", "concave_points3", "area2"], axis=1, inplace=True)

# Standardize the feature variables
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X)
# Show the scaled data
print("\nScaled data (first 5 rows of X_train):\n", pd.DataFrame(X_train_scaled, columns=X.columns).head())

# Apply PCA for dimensionality reduction
pca = PCA(n_components=2, random_state=42)
X_pca = pca.fit_transform(X_train_scaled)

# Visualize PCA components
pca_df = pd.DataFrame(X_pca, columns=["PC1", "PC2"])
pca_df["Target"] = y['Diagnosis']

plt.figure(figsize=(8, 6))
sns.scatterplot(x="PC1", y="PC2", hue="Target", data=pca_df, palette="Set1", alpha=0.7)
plt.title("PCA Visualization")
plt.xlabel("Principal Component 1")
plt.ylabel("Principal Component 2")
plt.legend(title="Target", labels=["Benign", "Malignant"])
plt.grid()
plt.show()

# Detect outliers using Z-score
z_scores = np.abs(zscore(X_pca))
outliers_removed = (z_scores < 3).all(axis=1)  # Keep rows with Z-score < 3
X_clean = X_pca[outliers_removed]
y_clean = y[outliers_removed]

print(f"Data after outlier removal: {X_clean.shape[0]} rows")

y_clean['Diagnosis'] = y_clean['Diagnosis'].apply(lambda x: 0 if x == 'B' else 1)

# Split the balanced data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_clean, y_clean, test_size=0.2, random_state=42)

"""model

Logistic Regression is a statistical method used for classifying data into two or more categories. It predicts the probability of an event occurring based on input features.
"""

# Develop a model for logistic regression
lg_rg = LogisticRegression(max_iter=1000, random_state=42, class_weight='balanced')
lg_rg.fit(X_train, y_train)

# Make predictions
y_train__rg = lg_rg.predict(X_train)
y_test__rg = lg_rg.predict(X_test)
y_test_prb_lg = lg_rg.predict_proba(X_test)[:, 1]
# Evaluate Logistic Regression model
train_accuracy = accuracy_score(y_train, y_train__rg )
test_accuracy = accuracy_score(y_test, y_test__rg)

print("Logistic Regression Training Accuracy:", train_accuracy)
print("Logistic Regression Test Accuracy:", test_accuracy)
print("\nClassification Report (Test Data):\n", classification_report(y_test,y_test__rg))

# Confusion matrix
conf_matrix = confusion_matrix(y_test, y_test__rg)

# Plot Confusion Matrix
disp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix, display_labels=lg_rg.classes_)
disp.plot(cmap="Blues")
plt.title("Confusion Matrix - Logistic Regression (Test Data)")
plt.show()

# Plot Training vs. Test Accuracy
labels = ['Training Accuracy', 'Test Accuracy']
accuracies = [train_accuracy, test_accuracy]

plt.bar(labels, accuracies, color=['blue', 'orange'])
plt.ylim(0.9, 1.0)  # Accuracy ranges from 0 to 1
plt.title('Logistic Regression: Training vs Test Accuracy')
plt.ylabel('Accuracy')
plt.show()

# ROC-AUC Score
roc_auc = roc_auc_score(y_test, y_test_prb_lg)
print(f"ROC-AUC Score: {roc_auc:.2f}")

# ROC Curve
fpr, tpr, _ = roc_curve(y_test, y_test_prb_lg)
plt.figure(figsize=(8, 6))  # Optional: set figure size for better visibility
plt.plot(fpr, tpr, label=f"ROC AUC = {roc_auc:.2f}")
plt.plot([0, 1], [0, 1], 'k--')  # Diagonal line
plt.title("ROC Curve - Logistic Regression")
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.legend(loc="lower right")
plt.show()

# Train Random Forest model with improved parameters
rnm_mod = RandomForestClassifier(
    n_estimators=50,
    max_depth=10,
    min_samples_leaf=5,
    min_samples_split=10,
    max_features=0.5,
    random_state=42,
    class_weight='balanced'
)
rnm_mod.fit(X_train, y_train)

# Make predictions on both training and test datasets
y_train_pred_rnm = rnm_mod.predict(X_train)
y_test_pred_rnm = rnm_mod.predict(X_test)
y_test_prob = rnm_mod.predict_proba(X_test)[:, 1]

# Evaluate Random Forest model
train_accuracy_rnm = accuracy_score(y_train, y_train_pred_rnm)
test_accuracy_rnm = accuracy_score(y_test, y_test_pred_rnm)

print("Random Forest Training Accuracy:", train_accuracy_rnm)
print("Random Forest Test Accuracy:", test_accuracy_rnm)
print("\nClassification Report (Test Data):\n", classification_report(y_test, y_test_pred_rnm))

# Confusion matrix for test data
conf_matrix_rnm = confusion_matrix(y_test, y_test_pred_rnm)
disp_rf = ConfusionMatrixDisplay(confusion_matrix=conf_matrix_rnm, display_labels=rnm_mod.classes_)
disp_rf.plot(cmap="Blues")
plt.title("Confusion Matrix - Random Forest (Test Data)")
plt.show()

# Plot Training vs. Test Accuracy
labels = ['Training Accuracy', 'Test Accuracy']
accuracies_rf = [train_accuracy_rnm, test_accuracy_rnm]
plt.bar(labels, accuracies_rf, color=['green', 'purple'])
plt.ylim(0.8, 1.0)
plt.title('Random Forest: Training vs Test Accuracy')
plt.ylabel('Accuracy')
plt.show()

# ROC-AUC Score
roc_auc_rf = roc_auc_score(y_test, y_test_pred_rnm)
print(f"ROC-AUC Score: {roc_auc_rf:.2f}")

# ROC Curve
fpr_rf, tpr_rf, _ = roc_curve(y_test, y_test_pred_rnm)
plt.plot(fpr_rf, tpr_rf, label=f"ROC AUC = {roc_auc_rf:.2f}")
plt.plot([0, 1], [0, 1], 'k--')

plt.title("ROC Curve - Random Forest")
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.legend(loc="lower right")
plt.show()

from imblearn.over_sampling import SMOTE

# Apply SMOTE to balance the classes in the training set
balance_sm = SMOTE(random_state=42)
X_train_smote, y_train_smote = balance_sm.fit_resample(X_train, y_train)

# Initialize and train the XGBoost classifier with class weight balancing
mde_xgbst = XGBClassifier(
    n_estimators=100,
    learning_rate=0.1,
    max_depth=5,
    random_state=42,
    eval_metric='mlogloss'
)

mde_xgbst.fit(X_train_smote, y_train_smote)

# Make predictions on both training and test datasets
y_train_pred_xgbst = mde_xgbst.predict(X_train_smote)
y_test_pred_xgbst = mde_xgbst.predict(X_test)
y_test_prob = mde_xgbst.predict_proba(X_test)[:, 1]  # Probabilities for the positive class

# Evaluate the XGBoost model
train_accuracy_xgbst = accuracy_score(y_train_smote, y_train_pred_xgbst)
test_accuracy_xgbst = accuracy_score(y_test, y_test_pred_xgbst)

print("XGBoost Training Accuracy:", train_accuracy_xgbst)
print("XGBoost Test Accuracy:", test_accuracy_xgbst)

print("\nClassification Report (Training Data):")
print(classification_report(y_train_smote, y_train_pred_xgbst))

print("\nClassification Report (Test Data):")
print(classification_report(y_test, y_test_pred_xgbst))

# Confusion matrix for test data
conf_matrix_xgbst = confusion_matrix(y_test, y_test_pred_xgbst)

# Plot Confusion Matrix
disp_xgb = ConfusionMatrixDisplay(confusion_matrix=conf_matrix_xgbst, display_labels=[0, 1])
disp_xgb.plot(cmap="Blues")
plt.title("Confusion Matrix - XGBoost (Test Data)")
plt.show()

# Plot Training vs. Test Accuracy
labels = ['Training Accuracy', 'Test Accuracy']
accuracies_xgb = [train_accuracy_xgbst, test_accuracy_xgbst]

plt.bar(labels, accuracies_xgb, color=['teal', 'orange'])
plt.ylim(0.8, 1.0)
plt.title('XGBoost: Training vs Test Accuracy')
plt.ylabel('Accuracy')
plt.show()

# ROC-AUC Score
roc_auc_xgb = roc_auc_score(y_test, y_test_prob)
print(f"ROC-AUC Score: {roc_auc_xgb:.2f}")

# ROC Curve
fpr_xgb, tpr_xgb, _ = roc_curve(y_test, y_test_prob)
plt.plot(fpr_xgb, tpr_xgb, label=f"ROC AUC = {roc_auc_xgb:.2f}")
plt.plot([0, 1], [0, 1], 'k--')  # Diagonal line
plt.title("ROC Curve - XGBoost")
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.legend(loc="lower right")
plt.show()

# Plot Model Accuracy Comparison (Training vs. Test Accuracy for all models)
models = ['Logistic Regression', 'Random Forest', 'XGBoost']
train_accuracies = [train_accuracy, train_accuracy_rf, train_accuracy_xgb]
test_accuracies = [test_accuracy, test_accuracy_rf, test_accuracy_xgb]

x = range(len(models))

plt.bar(x, train_accuracies, width=0.4, label='Training Accuracy', color='blue', align='center')
plt.bar(x, test_accuracies, width=0.4, label='Test Accuracy', color='orange', align='edge')

plt.xticks(x, models)
plt.ylim(0.8, 1.0)
plt.title("Model Accuracy Comparison")
plt.ylabel("Accuracy")
plt.legend()
plt.show()