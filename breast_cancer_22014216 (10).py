# -*- coding: utf-8 -*-
"""Breast cancer -22014216.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1KJ4MToFle_eGQMQ-mjaQFXeZVfoxlSix
"""

#Import libraries
!pip install ucimlrepo
import pandas as pd
import numpy as np
from ucimlrepo import fetch_ucirepo
from sklearn.decomposition import PCA
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier
from sklearn.metrics import classification_report, accuracy_score, confusion_matrix, ConfusionMatrixDisplay, roc_auc_score, roc_curve
from sklearn.ensemble import IsolationForest
import matplotlib.pyplot as plt
import seaborn as sns
from xgboost import XGBClassifier
from scipy.stats import zscore

"""Fetch and load dataset"""

# Open the UCI dataset on breast cancer.
breast_cancer_data = fetch_ucirepo(id=17)

# Extract the target and characteristics
brst_X = breast_cancer_data.data.features
brst_y = breast_cancer_data.data.targets

# Show the metadata for the dataset
print("Dataset Metadata:")
print(breast_cancer_data.metadata)
print(brst_X )
print(brst_y)

# Look for any missing data.
print("Missing values in brst_X:\n", brst_X.isnull().sum())
print("Missing values in brst_y:\n", brst_y.isnull().sum())

"""Exploratory Data Analysis"""

# Verify the distribution of the target class.
print("\nTarget class distribution:\n", brst_y.value_counts())

# Print datatypes
brst_X.dtypes

#summary statistics
brst_X.describe

# Analyse the distribution of target.
plt.figure(figsize=(6, 4))
sns.countplot(x=brst_y['Diagnosis'])
plt.title("Target Class Distribution (Benign vs Malignant)")
plt.xlabel("Diagnosis")
plt.ylabel("Count")
plt.show()

"""So it is evident that it has a class imbalance."""

# Show the 'radius1' distribution.
plt.figure(figsize=(8, 6))
sns.histplot(brst_X['radius1'], bins=30, kde=True, color='blue')
plt.xlabel("Radius1")
plt.ylabel("Frequency")
plt.show()

#Features to plot for the pairplot
features_to_plot = ['radius1', 'texture1', 'perimeter1', 'area1']
print(brst_X.columns)
pair_brst = brst_X[features_to_plot].join(brst_y["Diagnosis"])
sns.pairplot(pair_brst, hue='Diagnosis', palette="Set1", diag_kind="kde")
plt.suptitle("Pairplot of Selected Features", y=1.02)
plt.show()

# Correlation heatmap
plt.figure(figsize=(12, 10))
sns.heatmap(brst_X.corr(), cmap="coolwarm", annot=False, linewidths=0.5)
plt.title("Correlation Heatmap of Features")
plt.show()

# Calculate the correlation matrix
corr_matrix = brst_X.corr()

# Locate highly correlated pairings features
corr_brst = corr_matrix.unstack().sort_values(ascending=False)
corr_brst_pairs = corr_brst [(corr_brst .abs() > 0.8) & (corr_brst  < 1.0)]

print(corr_brst_pairs)

"""Preprocessing dataset"""

# Drop the highly related features
brst_X.drop(["radius1", "radius3", "compactness2", "concave_points3", "area2"], axis=1, inplace=True)

# Make the feature variables uniform.
brst_scl = StandardScaler()
brst_scl_trans = brst_scl.fit_transform(brst_X)
# Show the scaled data
print("\nScaled data:\n", pd.DataFrame(brst_scl_trans, columns=brst_X.columns).head())

# Apply PCA for dimensionality reduction
pca = PCA(n_components=2, random_state=42)
brst_pc = pca.fit_transform(brst_scl_trans)

# Visualize PCA components
pca_brst = pd.DataFrame(brst_pc, columns=["PC1", "PC2"])
pca_brst["Target"] = brst_y['Diagnosis']

plt.figure(figsize=(8, 6))
sns.scatterplot(x="PC1", y="PC2", hue="Target", data=pca_brst, palette="Set1", alpha=0.7)
plt.title("PCA Visualization")
plt.xlabel("Principal Component 1")
plt.ylabel("Principal Component 2")
plt.legend(title="Target", labels=["Benign", "Malignant"])
plt.grid()
plt.show()

# Detect outliers using Z-score
z_brst_scor = np.abs(zscore(brst_pc))
outliers_removed = (z_brst_scor < 3).all(axis=1)  # Keep rows with Z-score < 3
brst_X_cln = brst_pc[outliers_removed]
brst_y_cln = brst_y[outliers_removed]

print(f"Data after outlier removal: {brst_X_cln.shape[0]} rows")

#Map the diagnosis
brst_y_cln['Diagnosis'] = brst_y_cln['Diagnosis'].apply(lambda x: 0 if x == 'B' else 1)

# Create training and testing sets.
brst_tX, brst_tx, brst_ty, brst_tY = train_test_split(brst_X_cln, brst_y_cln, test_size=0.2, random_state=42)

"""model"""

# Develop a model for logistic regression
lg_rg = LogisticRegression(max_iter=1000, random_state=42, class_weight='balanced')
lg_rg.fit(brst_tX, brst_ty)

# Make predictions
y_train__rg = lg_rg.predict(brst_tX)
y_test__rg = lg_rg.predict(brst_tx)
y_test_prb_lg = lg_rg.predict_proba(brst_tx)[:, 1]
# Evaluate Logistic Regression model
train_accuracy = accuracy_score(brst_ty, y_train__rg )
test_accuracy = accuracy_score(brst_tY, y_test__rg)

print("Logistic Regression Training Accuracy:", train_accuracy)
print("Logistic Regression Test Accuracy:", test_accuracy)
print("\nClassification Report (Test Data):\n", classification_report(brst_tY,y_test__rg))

# Confusion matrix
conf_matrix = confusion_matrix(brst_tY, y_test__rg)

# Plot Confusion Matrix
disp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix, display_labels=lg_rg.classes_)
disp.plot(cmap="Blues")
plt.title("Confusion Matrix - Logistic Regression (Test Data)")
plt.show()

# Plot Training vs. Test Accuracy
labels = ['Training Accuracy', 'Test Accuracy']
accuracies = [train_accuracy, test_accuracy]

plt.bar(labels, accuracies, color=['blue', 'orange'])
plt.ylim(0.9, 1.0)  # Accuracy ranges from 0 to 1
plt.title('Logistic Regression: Training vs Test Accuracy')
plt.ylabel('Accuracy')
plt.show()

# ROC-AUC Score
roc_auc = roc_auc_score(brst_tY, y_test_prb_lg)
print(f"ROC-AUC Score: {roc_auc:.2f}")

# ROC Curve
fpr, tpr, _ = roc_curve(brst_tY, y_test_prb_lg)
plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, label=f"ROC AUC = {roc_auc:.2f}")
plt.plot([0, 1], [0, 1], 'k--')
plt.title("ROC Curve - Logistic Regression")
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.legend(loc="lower right")
plt.show()

# Train Random Forest model with improved parameters
rnm_mod = RandomForestClassifier(
    n_estimators=50,
    max_depth=10,
    min_samples_leaf=5,
    min_samples_split=10,
    max_features=0.5,
    random_state=42,
    class_weight='balanced'
)
rnm_mod.fit(brst_tX, brst_ty)

# Make predictions on both training and test datasets
y_train_pred_rnm = rnm_mod.predict(brst_tX)
y_test_pred_rnm = rnm_mod.predict(brst_tx)
y_test_prob = rnm_mod.predict_proba(brst_tx)[:, 1]

# Evaluate Random Forest model
train_accuracy_rnm = accuracy_score(brst_ty, y_train_pred_rnm)
test_accuracy_rnm = accuracy_score(brst_tY, y_test_pred_rnm)

print("Random Forest Training Accuracy:", train_accuracy_rnm)
print("Random Forest Test Accuracy:", test_accuracy_rnm)
print("\nClassification Report (Test Data):\n", classification_report(brst_tY, y_test_pred_rnm))

# Confusion matrix for test data
conf_matrix_rnm = confusion_matrix(brst_tY, y_test_pred_rnm)
disp_rf = ConfusionMatrixDisplay(confusion_matrix=conf_matrix_rnm, display_labels=rnm_mod.classes_)
disp_rf.plot(cmap="Blues")
plt.title("Confusion Matrix - Random Forest (Test Data)")
plt.show()

# Plot Training vs. Test Accuracy
labels = ['Training Accuracy', 'Test Accuracy']
accuracies_rf = [train_accuracy_rnm, test_accuracy_rnm]
plt.bar(labels, accuracies_rf, color=['green', 'purple'])
plt.ylim(0.8, 1.0)
plt.title('Random Forest: Training vs Test Accuracy')
plt.ylabel('Accuracy')
plt.show()

# ROC-AUC Score
roc_auc_rf = roc_auc_score(brst_tY, y_test_pred_rnm)
print(f"ROC-AUC Score: {roc_auc_rf:.2f}")

# ROC Curve
fpr_rf, tpr_rf, _ = roc_curve(brst_tY, y_test_pred_rnm)
plt.plot(fpr_rf, tpr_rf, label=f"ROC AUC = {roc_auc_rf:.2f}")
plt.plot([0, 1], [0, 1], 'k--')

plt.title("ROC Curve - Random Forest")
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.legend(loc="lower right")
plt.show()

from imblearn.over_sampling import SMOTE

# Apply SMOTE to balance the classes in the training set
balance_sm = SMOTE(random_state=42)
brst_smt_X, brst_smt_y = balance_sm.fit_resample(brst_tX, brst_ty)

# Initialize and train the XGBoost classifier with class weight balancing
mde_xgbst = XGBClassifier(
    n_estimators=100,
    learning_rate=0.1,
    max_depth=5,
    random_state=42,
    eval_metric='mlogloss'
)

mde_xgbst.fit(brst_smt_X, brst_smt_y)

# Make predictions on both training and test datasets
y_train_pred_xgbst = mde_xgbst.predict(brst_smt_X)
y_test_pred_xgbst = mde_xgbst.predict(brst_tx)
y_test_prob = mde_xgbst.predict_proba(brst_tx)[:, 1]  # Probabilities for the positive class

# Evaluate the XGBoost model
train_accuracy_xgbst = accuracy_score(brst_smt_y, y_train_pred_xgbst)
test_accuracy_xgbst = accuracy_score(brst_tY, y_test_pred_xgbst)

print("XGBoost Training Accuracy:", train_accuracy_xgbst)
print("XGBoost Test Accuracy:", test_accuracy_xgbst)

print("\nClassification Report (Training Data):")
print(classification_report(brst_smt_y, y_train_pred_xgbst))

print("\nClassification Report (Test Data):")
print(classification_report(brst_tY, y_test_pred_xgbst))

# Confusion matrix for test data
conf_matrix_xgbst = confusion_matrix(brst_tY, y_test_pred_xgbst)

# Plot Confusion Matrix
disp_xgb = ConfusionMatrixDisplay(confusion_matrix=conf_matrix_xgbst, display_labels=[0, 1])
disp_xgb.plot(cmap="Blues")
plt.title("Confusion Matrix - XGBoost (Test Data)")
plt.show()

# Plot Training vs. Test Accuracy
labels = ['Training Accuracy', 'Test Accuracy']
accuracies_xgb = [train_accuracy_xgbst, test_accuracy_xgbst]

plt.bar(labels, accuracies_xgb, color=['teal', 'orange'])
plt.ylim(0.8, 1.0)
plt.title('XGBoost: Training vs Test Accuracy')
plt.ylabel('Accuracy')
plt.show()

# ROC-AUC Score
roc_auc_xgb = roc_auc_score(brst_tY, y_test_prob)
print(f"ROC-AUC Score: {roc_auc_xgb:.2f}")

# ROC Curve
fpr_xgb, tpr_xgb, _ = roc_curve(brst_tY, y_test_prob)
plt.plot(fpr_xgb, tpr_xgb, label=f"ROC AUC = {roc_auc_xgb:.2f}")
plt.plot([0, 1], [0, 1], 'k--')  # Diagonal line
plt.title("ROC Curve - XGBoost")
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.legend(loc="lower right")
plt.show()

# Plot Model Accuracy Comparison (Training vs. Test Accuracy for all models)
models = ['Logistic Regression', 'Random Forest', 'XGBoost']
train_accuracies = [train_accuracy, train_accuracy_rnm, train_accuracy_xgbst]
test_accuracies = [test_accuracy, test_accuracy_rnm, test_accuracy_xgbst]

x = range(len(models))

plt.bar(x, train_accuracies, width=0.4, label='Training Accuracy', color='blue', align='center')
plt.bar(x, test_accuracies, width=0.4, label='Test Accuracy', color='orange', align='edge')

plt.xticks(x, models)
plt.ylim(0.8, 1.0)
plt.title("Model Accuracy Comparison")
plt.ylabel("Accuracy")
plt.legend()
plt.show()